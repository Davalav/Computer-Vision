{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs) and Transfer learning - D7046E @ LTU.SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This third and last ANN exercise includes two parts: the first part is an introduction to CNNs and transfer learning, and the second part introduces the **ANN project**. During the third course week you should familiarise yourself with the second part describing the project, so that you can select if you want to perform the ANN *or* SNN project. After completing ANN 2, all students should complete this exercise.\n",
    "\n",
    "The goal of this exercise is to understand what convolution is and how convolution is used in artificial neural networks that can benefit from translation invariance in some of the input dimensions, for example in image and sound processing. The exercise will also teach you about transfer learning, fine-tuning and feature extraction.\n",
    "\n",
    "## Literature\n",
    "This exercise will rely on the following sections in the [course book](https://www.deeplearningbook.org/).\n",
    "\n",
    "- Chapter 9\n",
    "    - Most of it\n",
    "- Chapter 7\n",
    "    - Section 7.4 - Dataset augmentation\n",
    "- Chapter 15\n",
    "    - Section 15.2 - Transfer learning\n",
    "    \n",
    "## Examination\n",
    "Epochs are predefined below to 30. Feel free to increase/decrease this number depending on the hardware that you are working with. Just make sure that you use the same hyperparameters on tasks 2, 3 and 4. **Make sure you have all examination requirements described below prepared before presenting.**\n",
    "\n",
    "### Task 1\n",
    "1. Implementation of \"same convolution\".\n",
    "2. The resulting image using 3 different filters.\n",
    "\n",
    "### Task 2\n",
    "1. The given network trained, validated and tested on the given dataset. Don't forget to make the train/validation/test split of the dataset. This can be achieved programmatically using https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split.\n",
    "2. Some type of regularization should be used. You should understand how the chosen regularization technique works.\n",
    "3. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "4. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "5. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?\n",
    "\n",
    "### Task 3\n",
    "1. Fine-tune Resnet18 on the given dataset.\n",
    "2. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "3. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "4. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?\n",
    "\n",
    "### Task 4\n",
    "1. Use Resnet18 as a feature extractor on the dataset.\n",
    "2. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "3. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "4. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Before diving into the implementation, it's crucial to develop an intuitive understanding of how CNNs work and why they're so effective for image-related tasks.\n",
    "\n",
    "## Visual and Interactive Resources\n",
    "\n",
    "To develop your intuition for CNNs, we strongly recommend exploring these interactive visualizations and animations:\n",
    "\n",
    "- **[CNN Explainer](https://poloclub.github.io/cnn-explainer/)** - An interactive visualization that lets you see how CNNs process images in real-time. You can upload your own images and watch activations propagate through the network.\n",
    "- **[ConvNetJS CIFAR-10 Demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)** - Watch a CNN train in your browser and see how filters evolve during training.\n",
    "- **[3Blue1Brown: Convolutional Neural Networks](https://www.youtube.com/watch?v=KuXjwB4LzSA)** - Visual breakdown of convolution operations.\n",
    "- **[Convolution Arithmetic](https://github.com/vdumoulin/conv_arithmetic)** - Animated GIFs showing different convolution operations (padding, strides, dilations).\n",
    "\n",
    "## Key Concepts: Why CNNs Work\n",
    "\n",
    "### 1. **Local Connectivity & Parameter Sharing**\n",
    "Traditional fully-connected layers would require millions of parameters for even small images (e.g., a 224×224 RGB image = 150,528 input features). CNNs solve this by:\n",
    "- **Local receptive fields**: Each neuron only looks at a small patch of the image\n",
    "- **Weight sharing**: The same filter (kernel) slides across the entire image, dramatically reducing parameters\n",
    "- **Translation invariance**: A feature detector works anywhere in the image\n",
    "\n",
    "### 2. **Hierarchical Feature Learning**\n",
    "CNNs automatically learn a hierarchy of features:\n",
    "```\n",
    "Early layers → Simple features (edges, colors, textures)\n",
    "Middle layers → Patterns (corners, shapes, simple objects)\n",
    "Deep layers → Complex features (eyes, wheels, faces)\n",
    "Output layer → High-level concepts (dog, car, airplane)\n",
    "```\n",
    "\n",
    "### 3. **The Convolution Operation**\n",
    "When a filter (kernel) slides over an image:\n",
    "- It performs element-wise multiplication with the local patch\n",
    "- Sums the results to produce one output value\n",
    "- This detects specific patterns (like edges, corners, textures)\n",
    "- Different filters detect different features\n",
    "\n",
    "**Visualizing a 3×3 filter sliding over an image:**\n",
    "```\n",
    "Input:          Kernel:        Output:\n",
    "[1 2 3 4 5]     [1 0 1]       [13 16 19]\n",
    "[2 3 4 5 6]  ⊛  [0 1 0]   →   [18 21 24]\n",
    "[3 4 5 6 7]     [1 0 1]       [23 26 29]\n",
    "```\n",
    "\n",
    "The term \"same convolution\" refers to a type of convolution operation used in CNNs where padding (typically zeros) is added to the input data such that the output feature map retains the same spatial dimensions (height and width). The term \"convolution\" in CNNs is a common convention, but it differs from the strict mathematical definition. Formally, the mathematical convolution operator used in transform theory (e.g., Fourier transforms) involves reversing one of the signals, typically the kernel, before the element-wise multiplication and summation. By contrast, the \"convolution\" operator used in CNNs do not reverse the kernel and actually performs a so-called cross-correlation operation. This simplifies computation and backpropagation in deep learning frameworks like TensorFlow and PyTorch.\n",
    "\n",
    "### 4. **What Do Filters Actually Learn?**\n",
    "- **Layer 1**: Edge detectors (horizontal, vertical, diagonal), color blobs\n",
    "- **Layer 2**: Textures, simple shapes, corners\n",
    "- **Layer 3**: Object parts (wheels, eyes, windows)\n",
    "- **Layer 4+**: Full objects and complex patterns\n",
    "\n",
    "You can explore learned filters at: [Distill.pub - Feature Visualization](https://distill.pub/2017/feature-visualization/)\n",
    "\n",
    "## Additional References\n",
    "\n",
    "For deeper understanding, consult these resources:\n",
    "\n",
    "- **[Deep Learning Book - Chapter 9](https://www.deeplearningbook.org/)** - Convolutional Networks (comprehensive theoretical foundation)\n",
    "- **[CS231n Stanford Course](http://cs231n.stanford.edu/)** - Convolutional Neural Networks for Visual Recognition\n",
    "- **[Distill.pub](https://distill.pub/)** - High-quality, visual explanations of deep learning concepts\n",
    "- **[Understanding Convolutions](https://jcbgamboa.github.io/2017/08/12/what-are-convolutions/)** - Mathematical perspective with clear examples\n",
    "- **[A guide to convolution arithmetic](https://arxiv.org/abs/1603.07285)** - Technical deep dive into convolution mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution in Neural Networks\n",
    "A convolutional neural network, CNN for short, is a type of ANN that consists of at least one convolutional layer. CNN's are often used where the input size may vary such as when we are dealing with image input. The architecture of CNNs was inspired by how the visual cortex functions in our brain.\n",
    "\n",
    "## Task 1: Implement convolution\n",
    "Implement 2d same convolution without using a built-in convolution function. This should function as described in [this blog post](https://jcbgamboa.github.io/2017/08/12/what-are-convolutions/). One of the great strengths of convolution is that it functions on any sized image, hence it is important that your implementation also does. Same convolution means that the dimensions of the output are the same as the dimensions of the input. This is achieved by padding the input.\n",
    "\n",
    "Once you have implemented a function that performs 2d convolution, use that to perform convolution over all channels in this image. Show the result using 3 different filters.\n",
    "\n",
    "To find the padding needed to get the input to be the same space as the output you can use the formula:\n",
    "\n",
    "$$ n_{out} = \\left \\lfloor\\frac{n_{in}+2p-k}{s} \\right \\rfloor+1 $$\n",
    "\n",
    "where $n_{out}$ is the number of output features, $n_{in}$ is the number of input features, $k$ is the kernel size, $p$ is the padding size and $s$ is the stride size. You can assume that the stride is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 30]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Implement same convolution in the function below (kernel is a 2d numpy array an example of which can be found in the test)\n",
    "def conv(image, kernel, strides=1):\n",
    "    pass\n",
    "\n",
    "# Our test, don't edit\n",
    "inp = np.array([[1,1,1,1],[1,1,2,1],[1,-3,-4,1],[1,1,1,1]])\n",
    "kernel = np.array([[0,1,0],[1,2,1],[0,1,0]]) # This is the second input of conv()\n",
    "\n",
    "# If all are TRUE the convolution is implemented correctly\n",
    "ans = np.array([[4, 5, 6, 4], [5, 3, 3, 6], [1, -7, -7, 0], [4, 1, 0, 4]])\n",
    "print(conv(inp, kernel) == ans)\n",
    "\n",
    "f, axarr = plt.subplots(4,1)\n",
    "\n",
    "# How to load images using opencv\n",
    "image_path = \"Licentiate.jpg\" # add your file path here\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # gray scale so we dont have to deal with more than 1 channel\n",
    "\n",
    "# Define your 3 kernels\n",
    "kernel_1 = ....\n",
    "kernel_2 = ....\n",
    "kernel_3 = ....\n",
    "\n",
    "# Perform the convolution (might take a couple of seconds depending on the implementation)\n",
    "output1 = conv(image, kernel_1)\n",
    "output2 = conv(image, kernel_2)\n",
    "output3 = conv(image, kernel_3)\n",
    "\n",
    "# plot the loaded image and the 3 convoluted images\n",
    "axarr[0].imshow(image, cmap=\"gray\")\n",
    "axarr[1].imshow(output1, cmap=\"gray\")\n",
    "axarr[2].imshow(output2, cmap=\"gray\")\n",
    "axarr[3].imshow(output3, cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "Computer vision (CV) is a task within the computer science field that aim is to extract high-level information from static images or video. Such high-level information can be, but is not limited to:\n",
    "* Object detection - Detect and classify objects within input images\n",
    "* Anomaly detection - Detect anomalies in the input images\n",
    "* Semantic segmentation - Classify each pixel in the input image into different classes\n",
    "* Object recognition - Classifying an entire image depending on what it contains\n",
    "\n",
    "CV has been studied for multiple decades where early solutions used handwritten feature extractors to extract information from the input. However, with the increase of computing power together with the rise of deep learning algorithms, the main method used to solve CV problems is convolutional neural networks.\n",
    "\n",
    "In this exercise, we will be taking a closer look at object recognition by first using a randomly initialized network and then utilizing transfer learning. **The dataset that you should use for this exercise can be downloaded in Canvas**. It is a subset of [this dataset](https://data.caltech.edu/records/mzrjq-6wc02). Remember to split the data into separate training, validation and test set.\n",
    "\n",
    "## Task 2: Implement the missing code and train it on the given dataset.\n",
    "For task 2, implement the missing parts of the code below. The code should correctly train, validate and test the model. There are some comments guiding you through the process, however if something is unclear try to leverage the documentation for pytorch found [here](https://pytorch.org/docs/stable/index.html). You should also add some type of regularization into your model.\n",
    "\n",
    "Remember to check the examination requirements in the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward function in the network\n",
    "        pass\n",
    "\n",
    "# Implement a train model function so you can re_use it in task 3 and 4. \n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = ...\n",
    "SHUFFLE = ...\n",
    "LEARNING_RATE = ...\n",
    "\n",
    "# Train augmentations\n",
    "transforms = transforms.Compose([\n",
    "    # Add training augmentations here, remember: we do not want to transform the validation images.\n",
    "    # For information about augmentation see: https://pytorch.org/vision/stable/transforms.html\n",
    "])\n",
    "\n",
    "# Load the full dataset, perform the training/validation/test split and then load the subsets into dataloaders.\n",
    "# Remember that the training images should be augmentated.\n",
    "DATA_DIR = \"\" # Path to dataset\n",
    "\n",
    "train_loader = ...\n",
    "val_loader = ...\n",
    "\n",
    "# Load our network\n",
    "model = Net()\n",
    "\n",
    "# Define our loss function\n",
    "criterion = ....\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = ....\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model = ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "Transfer learning refers to the practice to use a model which has already been pre-trained on a large dataset to be able to solve task $T_1$, replace the output layer or a few of the upper layers within this model and retrain the model on a smaller dataset to be able to solve task $T_2$. Formally this can be described as the following:\n",
    "\n",
    "__Def 1:__ Let $D_s$ be the source domain and $T_s$ be the corresponding source task. Let $D_t$ be the target domain and $T_t$ be the corresponding target task. Let $f_t$ be the predictive function for $T_s$. Thus transfer learning aims to improve the learning of $f_t$ in $D_t$ using the already learned knowledge in $D_s$ and $T_s$ where $D_s \\neq D_t$ and $T_s \\neq T_t$.\n",
    "\n",
    "The benefit from using transfer learning is that we can train an accurate computer vision model with relatively small amounts of data and computing resources compared to the costly pretraining process of the full convolutional neural network (a few days using multiple GPUs). \n",
    "\n",
    "## Fine-tuning and Feature extraction\n",
    "There are two main ideas when it comes to transfer learning, fine-tuning and feature extraction. When using fine-tuning we allow all weights to be changed during the training phase. However, when we use the pre-trained model as a feature extractor we instead freeze earlier layers of the model, which means that the weights in those layers will not be updated during the training phase and we only update the weights in the upper layers that we have replaced. \n",
    "\n",
    "This works because low-level information extracted from the input image is universal between tasks, examples of such information is edge detection, shape detection and pattern detection. This is what the early layers are optimized to do, where later layers extract more abstract features relevant for the task. \n",
    "\n",
    "Most of the pre-trained models in PyTorch are trained on [ImageNet](http://www.image-net.org/). \n",
    "\n",
    "In this exercise, we use ResNet18 as our model. You should make yourself familiar with the Resnet18 architecture using, for example, [the paper](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "## Task 3: Fine-tuning\n",
    "In task 3 you should fine-tune Resnet18 to the small dataset which is provided above. Some code has been given to you. Remember to re-use functions (such as trained_model) from task 2 to decrease the implementation time.\n",
    "\n",
    "Remember to check the examination requirements at the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune a model to the dataset\n",
    "# We use resnet18 as the model.\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# Do the things required for fine-tuning before training the model\n",
    "\n",
    "# Train the model\n",
    "trained_model_ft = train_model(model_ft, criterion_ft, optimizer_ft, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model = ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Feature extraction\n",
    "In task 4, you should use Resnet18 as a feature extractor. Similarly to task 3, some code has been provided. Remember to re-use as much code as you can. \n",
    "\n",
    "Once again, check the examination requirements so you don't forget to implement some required functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a predefined model as a feature extractor\n",
    "\n",
    "# We use resnet18 as the model.\n",
    "model_fe = models.resnet18(pretrained=True)\n",
    "\n",
    "# Do the things required for fine-tuning before training the model\n",
    "\n",
    "# Train the model\n",
    "trained_model_fe = train_model(model_fe, criterion_fe, optimizer_fe, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model = ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Project: Audio Classification with CNN and RNN\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "For students who select the ANN project track, the goal is to classify audio commands from the **Google Speech Commands dataset** using both Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), and compare their robustness to varying levels of background \"noise\" sampled from another dataset.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "### Google Speech Commands Dataset\n",
    "The [Google Speech Commands dataset](https://pytorch.org/audio/stable/generated/torchaudio.datasets.SPEECHCOMMANDS.html) is a collection of short (1 second) audio clips of spoken words. The dataset contains 65,000 utterances of 30 short words by thousands of different speakers. Each audio file is sampled at 16 kHz.\n",
    "\n",
    "**Dataset characteristics:**\n",
    "- Sample rate: 16 kHz\n",
    "- Duration: 1 second per clip\n",
    "- Number of classes: 30 words (e.g., \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", etc.)\n",
    "- Total samples: ~65,000\n",
    "\n",
    "**How to access:** The dataset can be loaded directly using `torchaudio.datasets.SPEECHCOMMANDS` or downloaded from the [official repository](http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz).\n",
    "\n",
    "### ESC-50: Environmental Sound Classification\n",
    "The [ESC-50 dataset](https://github.com/karolpiczak/ESC-50) contains 2,000 environmental audio recordings organized into 50 classes (40 clips per class). For this project, you will use sounds from this dataset as background noise to test the robustness of your models.\n",
    "\n",
    "**Dataset characteristics:**\n",
    "- Sample rate: 44.1 kHz (needs resampling to 16 kHz before combining with a speech command)\n",
    "- Duration: 5 seconds per clip\n",
    "- Categories include: rain, sea waves, crackling fire, clock tick, dog bark, footsteps, door knock, etc.\n",
    "\n",
    "**How to access:** Download from the [ESC-50 GitHub repository](https://github.com/karolpiczak/ESC-50).\n",
    "\n",
    "## Project Task\n",
    "\n",
    "### Objective\n",
    "Compare the performance of CNN and RNN architectures for speech command classification under different levels of background noise. The models should have a similar number of trainable parameters to ensure a fair comparison.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### 1. Data Preprocessing\n",
    "- **Resample** the ESC-50 background sounds from 44.1 kHz to **16 kHz** to match the Google Speech Commands sample rate\n",
    "- Create augmented versions of the Speech Commands dataset by mixing in background noise at different signal-to-noise ratios\n",
    "\n",
    "#### 2. Noise Mixing\n",
    "Evaluate classification accuracy on the Speech Commands dataset with **three noise levels** based on signal RMS (Root Mean Square):\n",
    "\n",
    "- **0% noise:** Clean speech commands audio (no background noise added)\n",
    "- **10% noise:** RMS of background noise = 10% of speech command RMS\n",
    "- **50% noise:** RMS of background noise = 50% of speech command RMS (i.e., equal signal and noise power)\n",
    "\n",
    "The RMS mixing ensures consistent and measurable noise levels across all experiments.\n",
    "\n",
    "#### 3. Model Implementation\n",
    "Implement and train two model architectures:\n",
    "\n",
    "**a) Convolutional Neural Network (CNN)**\n",
    "- Process audio spectrograms (e.g., Mel-spectrograms, MFCCs) as 2D images\n",
    "- Use convolutional layers to extract features from time-frequency representations\n",
    "\n",
    "**b) Recurrent Neural Network (RNN)**\n",
    "- Process raw audio waveforms or frame-based features sequentially\n",
    "- Use LSTM or GRU layers to capture temporal dependencies\n",
    "\n",
    "**Important:** Both models should have approximately the same number of parameters for a fair comparison.\n",
    "\n",
    "#### 4. Performance Metrics\n",
    "For each model and noise level, report:\n",
    "- Training accuracy\n",
    "- Validation accuracy\n",
    "- Test accuracy\n",
    "- Confusion matrix\n",
    "\n",
    "#### 5. Minimum Accuracy Requirement\n",
    "Your models should achieve **at least 75% test accuracy** on the clean (0% noise) Speech Commands dataset. This ensures that your baseline model is working correctly before evaluating noise robustness.\n",
    "\n",
    "#### 6. Analysis\n",
    "Compare and discuss:\n",
    "- Which architecture (CNN or RNN) performs better at each noise level?\n",
    "- How does performance degrade as noise increases?\n",
    "- What are the computational trade-offs between the two approaches?\n",
    "- Which types of commands are most/least robust to noise?\n",
    "\n",
    "\n",
    "### Optional challenge (deepening)\n",
    "Can you combine CNN and RNN to make one model that performs better in terms of accuracy versus total parameter count? Which type of model performs best at very high noise level? What level of noise is too high for reliable classification?\n",
    "\n",
    "## Key Resources\n",
    "\n",
    "### PyTorch Documentation\n",
    "- **[torchaudio.datasets.SPEECHCOMMANDS](https://pytorch.org/audio/stable/generated/torchaudio.datasets.SPEECHCOMMANDS.html)** - Official documentation for loading the Speech Commands dataset\n",
    "- **[torchaudio.transforms](https://pytorch.org/audio/stable/transforms.html)** - Audio transformations including MelSpectrogram, MFCC, and Resample\n",
    "- **[PyTorch LSTM Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)** - For implementing RNN models\n",
    "- **[PyTorch Conv2d Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)** - For implementing CNN models\n",
    "\n",
    "### Useful Libraries\n",
    "- **torchaudio**: For audio loading, transformations (Mel-spectrogram, MFCC, resampling), and the SPEECHCOMMANDS dataset\n",
    "- **torch.nn**: For building CNN (Conv2d, MaxPool2d) and RNN (LSTM, GRU) architectures\n",
    "- **librosa**: Alternative library for audio feature extraction and processing\n",
    "- **numpy**: For signal processing, RMS calculations, and noise mixing\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Load the datasets**: Use `torchaudio.datasets.SPEECHCOMMANDS` for speech commands and download ESC-50 from GitHub\n",
    "2. **Explore the data**: Understand audio formats, sample rates, and class distributions\n",
    "3. **Implement preprocessing**: Create a pipeline for resampling ESC-50 sounds and mixing noise at different RMS levels\n",
    "4. **Design CNN architecture**: Start with a model that processes Mel-spectrograms or MFCCs as 2D images\n",
    "5. **Design RNN architecture**: Build an LSTM/GRU model with similar parameter count that processes sequential audio features\n",
    "6. **Train on clean data**: Ensure both models achieve at least 75% accuracy before adding noise\n",
    "7. **Evaluate on noisy data**: Test both models on 10% and 50% noise levels\n",
    "8. **Analyze and compare**: Create plots and confusion matrices to compare model performance\n",
    "\n",
    "## Submission\n",
    "\n",
    "Your project submission should include:\n",
    "- Complete Jupyter notebook with all code, visualizations, and analysis\n",
    "- Clear documentation of your model architectures (number of parameters for each)\n",
    "- Performance comparison plots (accuracy vs. noise level for both models)\n",
    "- Confusion matrices for at least the 0%, 10% and 50% noise conditions\n",
    "- Discussion of results and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
